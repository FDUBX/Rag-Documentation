# RAG Project - Documentation Technique

Projet RAG (Retrieval-Augmented Generation) pour acc√©der √† de la documentation technique via un chat, bas√© sur LangChain et Ollama.

## üìã Pr√©requis

1. **Python 3.8+**
2. **Ollama** install√© et en cours d'ex√©cution
3. Les mod√®les Ollama suivants :
   - `mxbai-embed-large` (pour les embeddings)
   - `llama3.2` (pour la g√©n√©ration)

## üöÄ Installation

1. **Cr√©er un environnement virtuel** (recommand√©) :
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# ou
source venv/bin/activate  # Linux/Mac
```

2. **Installer les d√©pendances** :
```bash
pip install -r requirements.txt
```

3. **Installer les mod√®les Ollama** :
```bash
ollama pull mxbai-embed-large
ollama pull llama3.2
```

## üìù Utilisation

### 1. Indexer les documents

Placez vos documents techniques dans le dossier `data/` (PDF, TXT, DOCX, MD).

Puis lancez l'indexation :
```bash
python ingest.py
```

Cette commande va :
- Charger tous les documents du dossier `data/`
- Les d√©couper en chunks
- G√©n√©rer les embeddings avec Ollama
- Cr√©er le vector store avec scikit-learn

**Options disponibles** :

- `--reset` : R√©initialise compl√®tement l'index (supprime le vector store existant avant d'indexer)
  ```bash
  python ingest.py --reset
  ```

- `--list-urls` : Liste toutes les URLs sauvegard√©es
  ```bash
  python ingest.py --list-urls
  ```

- `--reindex-urls` : R√©indexe toutes les URLs sauvegard√©es (utile apr√®s un reset)
  ```bash
  python ingest.py --reindex-urls
  ```

- `--reset --reindex-urls` : Reset puis r√©indexe automatiquement les URLs sauvegard√©es
  ```bash
  python ingest.py --reset --reindex-urls
  ```

> ‚ö†Ô∏è **Attention** : L'option `--reset` supprime tous les documents d√©j√† index√©s. Les URLs index√©es via l'interface web sont sauvegard√©es dans `vectorstore/indexed_urls.json` et peuvent √™tre r√©index√©es avec `--reindex-urls`.

### 2. Lancer l'interface web

```bash
python app.py
```

Ouvrez votre navigateur sur : http://localhost:5000

### 3. Utiliser le chat

Posez vos questions sur la documentation technique dans l'interface de chat. Le syst√®me va :
- Rechercher les passages pertinents dans vos documents
- G√©n√©rer une r√©ponse bas√©e sur ces passages
- Afficher les sources utilis√©es

## üìÅ Structure du projet

```
NewRag/
‚îú‚îÄ‚îÄ data/              # Documents √† indexer
‚îú‚îÄ‚îÄ vectorstore/       # Vector store sauvegard√©
‚îú‚îÄ‚îÄ templates/         # Templates HTML
‚îÇ   ‚îî‚îÄ‚îÄ chat.html      # Interface de chat
‚îú‚îÄ‚îÄ ingest.py          # Script d'indexation
‚îú‚îÄ‚îÄ app.py             # Application Flask
‚îú‚îÄ‚îÄ config.yaml        # Configuration
‚îú‚îÄ‚îÄ requirements.txt   # D√©pendances Python
‚îî‚îÄ‚îÄ README.md          # Ce fichier
```

## ‚öôÔ∏è Configuration

Modifiez `config.yaml` pour ajuster :
- Les chemins des dossiers
- La taille des chunks
- Le nombre de r√©sultats √† r√©cup√©rer
- Les mod√®les Ollama utilis√©s
- Les param√®tres de g√©n√©ration (temp√©rature, max_tokens, etc.)
- Les timeouts pour les appels Ollama
- Le niveau de logging
- Le mode debug (d√©sactiv√© automatiquement en production)

### Mode Production

Pour activer le mode production (d√©sactive automatiquement le debug), vous avez **3 options** :

#### Option 1 : Fichier `.env` (recommand√©)

Cr√©ez un fichier `.env` √† la racine du projet :
```bash
# .env
PRODUCTION=true
```

Ou utilisez la variable standard Flask :
```bash
# .env
FLASK_ENV=production
```

#### Option 2 : Variable d'environnement temporaire (session actuelle)

**Windows PowerShell :**
```powershell
$env:PRODUCTION="true"
python app.py
```

**Windows CMD :**
```cmd
set PRODUCTION=true
python app.py
```

**Linux/Mac :**
```bash
export PRODUCTION=true
python app.py
```

#### Option 3 : Variable d'environnement permanente

**Windows :**
- Ouvrez "Variables d'environnement" dans les param√®tres syst√®me
- Ajoutez `PRODUCTION` avec la valeur `true`

**Linux/Mac :**
Ajoutez dans `~/.bashrc` ou `~/.zshrc` :
```bash
export PRODUCTION=true
```

> üí° **Astuce** : Le fichier `.env` est le plus pratique car il est versionn√© dans `.gitignore` et peut √™tre personnalis√© par environnement.

### Logging

Les logs sont configur√©s dans `config.yaml` :
- **Niveau** : DEBUG, INFO, WARNING, ERROR, CRITICAL
- **Fichier** : `./logs/app.log` (rotation automatique)
- **Taille max** : 10MB par fichier
- **Backups** : 5 fichiers de backup

Pour d√©sactiver les logs fichier, mettez `file: null` dans la config.

## üîß Technologies utilis√©es

- **LangChain** : Framework pour les applications LLM
- **LangChain Community** : Int√©grations communautaires
- **LangChain Ollama** : Int√©gration avec Ollama
- **scikit-learn** : Vector store pour la recherche
- **Flask** : Framework web
- **Ollama** : Ex√©cution locale de mod√®les LLM

## üìö Formats de documents support√©s

- PDF (`.pdf`)
- Texte (`.txt`)
- Markdown (`.md`)
- Word (`.docx`, `.doc`)

## üêõ D√©pannage

### Erreur "Vector store non trouv√©"
Lancez d'abord `python ingest.py` pour indexer vos documents.

### Erreur "Ollama non accessible"
Assurez-vous qu'Ollama est en cours d'ex√©cution :
```bash
ollama serve
```

### Erreur de mod√®le
V√©rifiez que les mod√®les sont bien install√©s :
```bash
ollama list
```

## üìù Notes

- Le vector store est sauvegard√© dans `vectorstore/vectorstore.pkl`
- Pour r√©indexer apr√®s avoir ajout√© des documents, relancez `python ingest.py`
- Les embeddings sont g√©n√©r√©s via Ollama, ce qui peut prendre du temps pour de gros volumes
- Les URLs index√©es sont sauvegard√©es dans `vectorstore/indexed_urls.json`
- Les logs sont disponibles dans `logs/app.log` (si configur√©)
- Le mode debug est automatiquement d√©sactiv√© en production (variable `FLASK_ENV=production`)
- Les timeouts Ollama sont configurables dans `config.yaml` (section `ollama`)

