# RAG Project - Documentation Technique

Projet RAG (Retrieval-Augmented Generation) pour accÃ©der Ã  de la documentation technique via un chat, basÃ© sur LangChain et Ollama.

## ğŸ“‹ PrÃ©requis

1. **Python 3.8+**
2. **Ollama** installÃ© et en cours d'exÃ©cution
3. Les modÃ¨les Ollama suivants :
   - `mxbai-embed-large` (pour les embeddings)
   - `llama3.2` (pour la gÃ©nÃ©ration)

## ğŸš€ Installation

1. **CrÃ©er un environnement virtuel** (recommandÃ©) :
```bash
python -m venv venv
venv\Scripts\activate  # Windows
# ou
source venv/bin/activate  # Linux/Mac
```

2. **Installer les dÃ©pendances** :
```bash
pip install -r requirements.txt
```

3. **Installer les modÃ¨les Ollama** :
```bash
ollama pull mxbai-embed-large
ollama pull llama3.2
```

## ğŸ“ Utilisation

### 1. Indexer les documents

Placez vos documents techniques dans le dossier `data/` (PDF, TXT, DOCX, MD).

Puis lancez l'indexation :
```bash
python ingest.py
```

Cette commande va :
- Charger tous les documents du dossier `data/`
- Les dÃ©couper en chunks
- GÃ©nÃ©rer les embeddings avec Ollama
- CrÃ©er le vector store avec scikit-learn

**Options disponibles** :

- `--reset` : RÃ©initialise complÃ¨tement l'index (supprime le vector store existant avant d'indexer)
  ```bash
  python ingest.py --reset
  ```

- `--list-urls` : Liste toutes les URLs sauvegardÃ©es
  ```bash
  python ingest.py --list-urls
  ```

- `--reindex-urls` : RÃ©indexe toutes les URLs sauvegardÃ©es (utile aprÃ¨s un reset)
  ```bash
  python ingest.py --reindex-urls
  ```

- `--reset --reindex-urls` : Reset puis rÃ©indexe automatiquement les URLs sauvegardÃ©es
  ```bash
  python ingest.py --reset --reindex-urls
  ```

> âš ï¸ **Attention** : L'option `--reset` supprime tous les documents dÃ©jÃ  indexÃ©s. Les URLs indexÃ©es via l'interface web sont sauvegardÃ©es dans `vectorstore/indexed_urls.json` et peuvent Ãªtre rÃ©indexÃ©es avec `--reindex-urls`.

### 2. Lancer l'interface web

```bash
python app.py
```

Ouvrez votre navigateur sur : http://localhost:5000

### 3. Utiliser le chat

Posez vos questions sur la documentation technique dans l'interface de chat. Le systÃ¨me va :
- Rechercher les passages pertinents dans vos documents
- GÃ©nÃ©rer une rÃ©ponse basÃ©e sur ces passages
- Afficher les sources utilisÃ©es

## ğŸ“ Structure du projet

```
NewRag/
â”œâ”€â”€ data/              # Documents Ã  indexer
â”œâ”€â”€ vectorstore/       # Vector store sauvegardÃ©
â”œâ”€â”€ templates/         # Templates HTML
â”‚   â””â”€â”€ chat.html      # Interface de chat
â”œâ”€â”€ ingest.py          # Script d'indexation
â”œâ”€â”€ app.py             # Application Flask
â”œâ”€â”€ config.yaml        # Configuration
â”œâ”€â”€ requirements.txt   # DÃ©pendances Python
â””â”€â”€ README.md          # Ce fichier
```

## âš™ï¸ Configuration

Modifiez `config.yaml` pour ajuster :
- Les chemins des dossiers
- La taille des chunks
- Le nombre de rÃ©sultats Ã  rÃ©cupÃ©rer
- Les modÃ¨les Ollama utilisÃ©s
- Les paramÃ¨tres de gÃ©nÃ©ration (tempÃ©rature, etc.)

## ğŸ”§ Technologies utilisÃ©es

- **LangChain** : Framework pour les applications LLM
- **LangChain Community** : IntÃ©grations communautaires
- **LangChain Ollama** : IntÃ©gration avec Ollama
- **scikit-learn** : Vector store pour la recherche
- **Flask** : Framework web
- **Ollama** : ExÃ©cution locale de modÃ¨les LLM

## ğŸ“š Formats de documents supportÃ©s

- PDF (`.pdf`)
- Texte (`.txt`)
- Markdown (`.md`)
- Word (`.docx`, `.doc`)

## ğŸ› DÃ©pannage

### Erreur "Vector store non trouvÃ©"
Lancez d'abord `python ingest.py` pour indexer vos documents.

### Erreur "Ollama non accessible"
Assurez-vous qu'Ollama est en cours d'exÃ©cution :
```bash
ollama serve
```

### Erreur de modÃ¨le
VÃ©rifiez que les modÃ¨les sont bien installÃ©s :
```bash
ollama list
```

## ğŸ“ Notes

- Le vector store est sauvegardÃ© dans `vectorstore/vectorstore.pkl`
- Pour rÃ©indexer aprÃ¨s avoir ajoutÃ© des documents, relancez `python ingest.py`
- Les embeddings sont gÃ©nÃ©rÃ©s via Ollama, ce qui peut prendre du temps pour de gros volumes

